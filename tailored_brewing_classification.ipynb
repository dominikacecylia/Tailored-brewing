{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit (conda)"
  },
  "interpreter": {
   "hash": "f8b6ea6d13f5748502004c2e5659a6272de46c202ad482ac9f90784c65a6c667"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Learner selection\n",
    "\n",
    "Having analysed and modelled coffee data, it is time to select the approriate Learner for it. \n",
    "\n",
    "**Learning objective:** From my analysis I want to select learner, which will most accuratelly fit the data and predict brewing method, focusing on avoiding false positive (could be brand damaging to the client), but also considering false negative (not to miss out on cheaper raw materials).\n",
    "\n",
    "\n",
    "To chose the learner I take into cosideration the following factors: all my data is labelled categorical text data, and I have only 851 samples after data cleaning and removal of outliers.\n",
    "\n",
    "Because of small volume of data I will try *SGD Classifier*, which relies on simple stochastic descent learning routine and is easy to implement.\n",
    "Because of categorical text data I will also test *Naive Bayes*, which works well with small training data, also it is good for combating the curse of dimensionality (though it does not seem to be the problem in my dataset).\n",
    "Another Classifier I will try is the *KNN*, which works well where the decision boundry is very irregular.\n",
    "Finally, I am considering *Decistion Trees*, however, their implemention in sklear requires numerical data only, and I will have to encode all of my text variables.\n",
    "\n",
    "My initial assumption is that Naive Bayes will best correspond to my data, and the problem I am trying to solve."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data split\n",
    "\n",
    "Before I can start testing learners, I need to split my data. I will use random seed 42, to make sure I always get the same randomly divided data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    origin_region  natural  fermented_traditional  fermented_closed_tank  \\\n",
       "idx                                                                        \n",
       "1           Latam     True                  False                  False   \n",
       "2          Africa     True                  False                  False   \n",
       "3          Africa    False                  False                  False   \n",
       "4            Asia     True                  False                  False   \n",
       "5           Latam     True                  False                  False   \n",
       "\n",
       "     brewing_method_binary_num  \n",
       "idx                             \n",
       "1                            1  \n",
       "2                            1  \n",
       "3                            1  \n",
       "4                            1  \n",
       "5                            1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>origin_region</th>\n      <th>natural</th>\n      <th>fermented_traditional</th>\n      <th>fermented_closed_tank</th>\n      <th>brewing_method_binary_num</th>\n    </tr>\n    <tr>\n      <th>idx</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Latam</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Africa</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Africa</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Asia</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Latam</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "coffee_df = pd.read_csv('data\\coffee_desk_dataset_ead_selected.csv', index_col=0)\n",
    "coffee_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = coffee_df.drop('brewing_method_binary_num', axis=1) # defining predictors\n",
    "y_df = coffee_df['brewing_method_binary_num'] # defining target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=42) #using random state to ensure I always have random division with the same random numbers\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "source": [
    "## Data encoding\n",
    "\n",
    "Since my data is categorical text data, I will encode it with one hot encoder into binary vectors."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "OneHotEncoder(handle_unknown='ignore')"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "encoder.fit(X_train) # all variables are categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = encoder.transform(X_train)\n",
    "X_validation = encoder.transform(X_validation)\n",
    "X_test = encoder.transform(X_test)"
   ]
  },
  {
   "source": [
    "## Model selection\n",
    "\n",
    "Having encoded data (target is already encoded as 1 for specialty brewing and 0 for espresso), I can now proceed to fit different models on my data, and decide which to use as the optimal with possible changes to optimazation and hyperparameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CategoricalNB()\n",
    "X_train_dense = X_train.todense()\n",
    "X_validation_dense = X_validation.todense()\n",
    "model.fit(X_train_dense, y_train)\n",
    "y_train_pred = model.predict(X_train_dense)\n",
    "y_validation_pred = model.predict(X_validation_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True negatives: 28\nTrue positives: 31\nFalse negatives: 12\nFalse positives: 14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_validation, y_validation_pred).ravel()\n",
    "print(f'True negatives: {tn}')\n",
    "print(f'True positives: {tp}')\n",
    "print(f'False negatives: {fn}')\n",
    "print(f'False positives: {fp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True negatives: 209\nTrue positives: 258\nFalse negatives: 103\nFalse positives: 110\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_train, y_train_pred).ravel()\n",
    "print(f'True negatives: {tn}')\n",
    "print(f'True positives: {tp}')\n",
    "print(f'False negatives: {fn}')\n",
    "print(f'False positives: {fp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'CategoricalBayes' : CategoricalNB(),\n",
    "    'SGDClassifier' : SGDClassifier(),\n",
    "    'KNNs' : KNeighborsClassifier(n_neighbors=9)\n",
    "}\n",
    "\n",
    "predictions_by_model = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_validation_pred = model.predict(X_validation)\n",
    "\n",
    "    predictions_by_model[name] = y_test_pred"
   ]
  }
 ]
}